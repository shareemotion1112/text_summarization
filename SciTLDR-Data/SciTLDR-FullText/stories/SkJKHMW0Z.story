Humans possess an ability to abstractly reason about objects and their interactions, an ability not shared with state-of-the-art deep learning models.

Relational networks, introduced by Santoro et al. (2017), add the capacity for relational reasoning to deep neural networks, but are limited in the complexity of the reasoning tasks they can address.

We introduce recurrent relational networks which increase the suite of solvable tasks to those that require an order of magnitude more steps of relational reasoning.

We use recurrent relational networks to solve Sudoku puzzles and achieve state-of-the-art results by solving 96.6% of the hardest Sudoku puzzles, where relational networks fail to solve any.

We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with state-of-the-art sparse differentiable neural computers.

The recurrent relational network is a general purpose module that can augment any neural network model with the capacity to do many-step relational reasoning.

@highlight

We introduce Recurrent Relational Networks, a powerful and general neural network module for relational reasoning, and use it to solve 96.6% of the hardest Sudokus and 19/20 BaBi tasks.

@highlight

Introduced recurrent relational network (RRNs) that can be added to any neural networks to add relational reasoning capacity.

@highlight

Introduction of a deep neural network for structured prediction that achieves state-of-the-art performance on Soduku puzzles and the BaBi task.

@highlight

This paper describes a method called relational network to add relational reasoning capacity to deep neural networks.
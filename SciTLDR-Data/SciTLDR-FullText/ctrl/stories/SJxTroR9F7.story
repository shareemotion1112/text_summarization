We propose a new sample-efficient methodology, called Supervised Policy Update (SPU), for deep reinforcement learning.

Starting with data generated by the current policy, SPU formulates and solves a constrained optimization problem in the non-parameterized proximal policy space.

Using supervised regression, it then converts the optimal non-parameterized policy to a parameterized policy, from which it draws new samples.

The methodology is general in that it applies to both discrete and continuous action spaces, and can handle a wide variety of proximity constraints for the non-parameterized optimization problem.

We show how the Natural Policy Gradient and Trust Region Policy Optimization (NPG/TRPO) problems, and the Proximal Policy Optimization (PPO) problem can be addressed by this methodology.

The SPU implementation is much simpler than TRPO.

In terms of sample efficiency, our extensive experiments show SPU outperforms TRPO in Mujoco simulated robotic tasks and outperforms PPO in Atari video game tasks.

The policy gradient problem in deep reinforcement learning (DRL) can be defined as seeking a parameterized policy with high expected reward.

An issue with policy gradient methods is poor sample efficiency BID10 BID21 BID27 BID29 .

In algorithms such as REINFORCE BID28 , new samples are needed for every gradient step.

When generating samples is expensive (such as robotic environments), sample efficiency is of central concern.

The sample efficiency of an algorithm is defined to be the number of calls to the environment required to attain a specified performance level BID10 .Thus, given the current policy and a fixed number of trajectories (samples) generated, the goal of the sample efficiency problem is to construct a new policy with the highest performance improvement possible.

To do so, it is desirable to limit the search to policies that are close to the original policy π θ k BID21 BID29 BID24 .

Intuitively, if the candidate new policy π θ is far from the original policy π θ k , it may not perform better than the original policy because too much emphasis is being placed on the relatively small batch of new data generated by π θ k , and not enough emphasis is being placed on the relatively large amount of data and effort previously used to construct π θ k .This guideline of limiting the search to nearby policies seems reasonable in principle, but requires a distance η(π θ , π θ k ) between the current policy π θ k and the candidate new policy π θ , and then attempt to solve the constrained optimization problem: DISPLAYFORM0 subject to η(π θ , π θ k ) ≤ δwhereĴ(π θ | π θ k , new data) is an estimate of J(π θ ), the performance of policy π θ , based on the previous policy π θ k and the batch of fresh data generated by π θ k .

The objective (1) attempts to maximize the performance of the updated policy, and the constraint (2) ensures that the updated policy is not too far from the policy π θ k that was used to generate the data.

Several recent papers BID21 BID24 belong to the framework (1)-(2).Our work also strikes the right balance between performance and simplicity.

The implementation is only slightly more involved than PPO .

Simplicity in RL algorithms has its own merits.

This is especially useful when RL algorithms are used to solve problems outside of traditional RL testbeds, which is becoming a trend BID30 BID16 .We propose a new methodology, called Supervised Policy Update (SPU), for this sample efficiency problem.

The methodology is general in that it applies to both discrete and continuous action spaces, and can address a wide variety of constraint types for (2).

Starting with data generated by the current policy, SPU optimizes over a proximal policy space to find an optimal non-parameterized policy.

It then solves a supervised regression problem to convert the non-parameterized policy to a parameterized policy, from which it draws new samples.

We develop a general methodology for finding an optimal policy in the non-parameterized policy space, and then illustrate the methodology for three different definitions of proximity.

We also show how the Natural Policy Gradient and Trust Region Policy Optimization (NPG/TRPO) problems and the Proximal Policy Optimization (PPO) problem can be addressed by this methodology.

While SPU is substantially simpler than NPG/TRPO in terms of mathematics and implementation, our extensive experiments show that SPU is more sample efficient than TRPO in Mujoco simulated robotic tasks and PPO in Atari video game tasks.

Off-policy RL algorithms generally achieve better sample efficiency than on-policy algorithms BID8 .

However, the performance of an on-policy algorithm can usually be substantially improved by incorporating off-policy training BID17 , BID26 ).

Our paper focuses on igniting interests in separating finding the optimal policy into a two-step process: finding the optimal non-parameterized policy, and then parameterizing this optimal policy.

We also wanted to deeply understand the on-policy case before adding off-policy training.

We thus compare with algorithms operating under the same algorithmic constraints, one of which is being on-policy.

We leave the extension to off-policy to future work.

We do not claim state-of-the-art results.

We consider a Markov Decision Process (MDP) with state space S, action space A, and reward function r(s, a), s ∈ S, a ∈ A. Let π = {π(a|s) : s ∈ S, a ∈ A} denote a policy, let Π be the set of all policies, and let the expected discounted reward be: DISPLAYFORM0 where γ ∈ (0, 1) is a discount factor and τ = (s 0 , a 0 , s 1 , . . . ) is a sample trajectory.

Let A π (s, a) be the advantage function for policy π BID14 .

Deep reinforcement learning considers a set of parameterized policies Π DL = {π θ |θ ∈ Θ} ⊂ Π, where each policy is parameterized by a neural network called the policy network.

In this paper, we will consider optimizing over the parameterized policies in Π DL as well as over the non-parameterized policies in Π. For concreteness, we assume that the state and action spaces are finite.

However, our methodology also applies to continuous state and action spaces, as shown in the Appendix.

One popular approach to maximizing J(π θ ) over Π DL is to apply stochastic gradient ascent.

The gradient of J(π θ ) evaluated at a specific θ = θ k can be shown to be BID28 : DISPLAYFORM1 We can approximate (4) by sampling N trajectories of length T from π θ k : DISPLAYFORM2 for the the future state probability distribution for policy π, and denote π(·|s) for the probability distribution over the action space A when in state s and using policy π.

Further denote D KL (π π θ k )[s] for the KL divergence from π(·|s) to π θ k (·|s), and denote the following as the "aggregated KL divergence".

DISPLAYFORM3

For the sample efficiency problem, the objective J(π θ ) is typically approximated using samples generated from π θ k BID21 .

Two different approaches are typically used to approximate J(π θ ) − J(π θ k ).

We can make a first order approximation of J(π θ ) around θ k BID18 BID29 BID21 : DISPLAYFORM0 where g k is the sample estimate (5).

The second approach is to approximate the state distribution BID1 : DISPLAYFORM1 DISPLAYFORM2 There is a well-known bound for the approximation (8) BID11 .

Furthermore, the approximation DISPLAYFORM3 to the first order with respect to the parameter θ .

Natural gradient (Amari, 1998) was first introduced to policy gradient by and then in BID18 BID29 BID1 BID21 . referred to collectively here as NPG/TRPO.

Algorithmically, NPG/TRPO finds the gradient update by solving the sample efficiency problem (1)-(2) with η(π θ , π θ k ) =D KL (π θ π θ k ), i.e., use the aggregate KL-divergence for the policy proximity constraint (2).

NPG/TRPO addresses this problem in the parameter space θ ∈ Θ. First, it approximates J(π θ ) with the first-order approximation (7) andD KL (π θ π θ k ) using a similar second-order method.

Second, it uses samples from π θ k to form estimates of these two approximations.

Third, using these estimates (which are functions of θ), it solves for the optimal θ * .

The optimal θ * is a function of g k and of h k , the sample average of the Hessian evaluated at θ k .

TRPO also limits the magnitude of the update to ensureD KL (π θ π θ k ) ≤ δ (i.e., ensuring the sampled estimate of the aggregated KL constraint is met without the second-order approximation).SPU takes a very different approach by first (i) posing and solving the optimization problem in the non-parameterized policy space, and then (ii) solving a supervised regression problem to find a parameterized policy that is near the optimal non-parameterized policy.

A recent paper, Guided Actor Critic (GAC), independently proposed a similar decomposition BID24 .

However, GAC is much more restricted in that it considers only one specific constraint criterion (aggregated reverse-KL divergence) and applies only to continuous action spaces.

Furthermore, GAC incurs significantly higher computational complexity, e.g. at every update, it minimizes the dual function to obtain the dual variables using SLSQP.

MPO also independently propose a similar decomposition BID24 .

MPO uses much more complex machinery, namely, Expectation Maximization to address the DRL problem.

However, MPO has only demonstrates preliminary results on problems with discrete actions whereas our approach naturally applies to problems with either discrete or continuous actions.

In both GAC and MPO, working in the non-parameterized space is a by-product of applying the main ideas in those papers to DRL.

Our paper demonstrates that the decomposition alone is a general and useful technique for solving constrained policy optimization.

Clipped-PPO ) takes a very different approach to TRPO.

At each iteration, PPO makes many gradient steps while only using the data from π θ k .

Without the clipping, PPO is the approximation (8).

The clipping is analogous to the constraint (2) in that it has the goal of keeping π θ close to π θ k .

Indeed, the clipping keeps π θ (a t |s t ) from becoming neither much larger than (1 + )π θ k (a t |s t ) nor much smaller than (1 − )π θ k (a t |s t ).

Thus, although the clipped PPO objective does not squarely fit into the optimization framework FORMULA0 - FORMULA1 , it is quite similar in spirit.

We note that the PPO paper considers adding the KL penalty to the objective function, whose gradient is similar to ours.

However, this form of gradient was demonstrated to be inferior to Clipped-PPO.

To the best of our knowledge, it is only until our work that such form of gradient is demonstrated to outperform Clipped-PPO.Actor-Critic using Kronecker-Factored Trust Region (ACKTR) BID29 proposed using Kronecker-factored approximation curvature (K-FAC) to update both the policy gradient and critic terms, giving a more computationally efficient method of calculating the natural gradients.

ACER BID26 ) exploits past episodes, linearizes the KL divergence constraint, and maintains an average policy network to enforce the KL divergence constraint.

In future work, it would of interest to extend the SPU methodology to handle past episodes.

In contrast to bounding the KL divergence on the action distribution as we have done in this work, Relative Entropy Policy Search considers bounding the joint distribution of state and action and was only demonstrated to work for small problems (Jan Peters, 2010).

The SPU methodology has two steps.

In the first step, for a given constraint criterion η(π, π θ k ) ≤ δ, we find the optimal solution to the non-parameterized problem: DISPLAYFORM0 Note that π is not restricted to the set of parameterized policies Π DL .

As commonly done, we approximate the objective function (8).

However, unlike PPO/TRPO, we are not approximating the constraint (2).

We will show below the optimal solution π * for the non-parameterized problem (9)-(10) can be determined nearly in closed form for many natural constraint criteria η(π, π θ k ) ≤ δ.

In the second step, we attempt to find a policy π θ in the parameterized space Π DL that is close to the target policy π * .

Concretely, to advance from θ k to θ k+1 , we perform the following steps:(i) We first sample N trajectories using policy π θ k , giving sample data DISPLAYFORM1 Here A i is an estimate of the advantage value A π θ k (s i , a i ). (For simplicity, we index the samples with i rather than with (i, t) corresponding to the tth sample in the ith trajectory.)(ii) For each s i , we define the target distribution π * to be the optimal solution to the constrained optimization problem (9)-(10) for a specific constraint η.(iii) We then fit the policy network π θ to the target distributions π * (·|s i ), i = 1, .., m. Specifically, to find θ k+1 , we minimize the following supervised loss function: DISPLAYFORM2 For this step, we initialize with the weights for π θ k .

We minimize the loss function L(θ) with stochastic gradient descent methods.

The resulting θ becomes our θ k+1 .

To illustrate the SPU methodology, for three different but natural types of proximity constraints, we solve the corresponding non-parameterized optimization problem and derive the resulting gradient for the SPU supervised learning problem.

We also demonstrate that different constraints lead to very different but intuitive forms of the gradient update.

We first consider constraint criteria of the form: DISPLAYFORM0 subject to DISPLAYFORM1 Note that this problem is equivalent to minimizing L π θ k (π) subject to the constraints FORMULA0 and FORMULA0 .

We refer to (13) as the "aggregated KL constraint" and to FORMULA0 as the "disaggregated KL constraint".

These two constraints taken together restrict π from deviating too much from π θ k .

We shall refer to FORMULA0 - FORMULA0 as the forward-KL non-parameterized optimization problem.

Note that this problem without the disaggregated constraints is analogous to the TRPO problem.

The TRPO paper actually prefers enforcing the disaggregated constraint to enforcing the aggregated constraints.

However, for mathematical conveniences, they worked with the aggregated constraints: "While it is motivated by the theory, this problem is impractical to solve due to the large number of constraints.

Instead, we can use a heuristic approximation which considers the average KL divergence" BID21 .

The SPU framework allows us to solve the optimization problem with the disaggregated constraints exactly.

Experimentally, we compared against TRPO in a controlled experimental setting, e.g. using the same advantage estimation scheme, etc.

Since we clearly outperform TRPO, we argue that SPU's two-process procedure has significant potentials.

DISPLAYFORM2 Note that π λ (a|s) is a function of λ.

Further, for each s, let λ s be such that DISPLAYFORM3 Theorem 1 The optimal solution to the problem (12)- FORMULA0 is given by: DISPLAYFORM4 where λ is chosen so that DISPLAYFORM5 Equation FORMULA0 provides the structure of the optimal non-parameterized policy.

As part of the SPU framework, we then seek a parameterized policy π θ that is close toπ λ (a|s), that is, minimizes the loss function (11).

For each sampled state s i , a straightforward calculation shows (Appendix B): DISPLAYFORM6 where DISPLAYFORM7 We estimate the expectation in (16) with the sampled action a i and approximate A π θ k (s i , a i ) as A i (obtained from the critic network), giving: DISPLAYFORM8 To simplify the algorithm, we slightly modify (17).

We replace the hyper-parameter δ with the hyper-parameter λ and tune λ rather than δ.

Further, we set ∼ λ si = λ for all s i in (17) and introduce per-state acceptance to enforce the disaggregated constraints, giving the approximate gradient: DISPLAYFORM9 We make the approximation that the disaggregated constraints are only enforced on the states in the sampled trajectories.

We use (18) as our gradient for supervised training of the policy network.

The equation FORMULA0 has an intuitive interpretation: the gradient represents a trade-off between the approximate performance of π θ (as captured by 1 λ DISPLAYFORM10 For the stopping criterion, we train until DISPLAYFORM11

In a similar manner, we can derive the structure of the optimal policy when using the reverse KL-divergence as the constraint.

For simplicity, we provide the result for when there are only disaggregated constraints.

We seek to find the non-parameterized optimal policy by solving: DISPLAYFORM0 DISPLAYFORM1 Theorem 2 The optimal solution to the problem (19)- FORMULA1 is given by: DISPLAYFORM2 where λ(s) > 0 and DISPLAYFORM3 Note that the structure of the optimal policy with the backward KL constraint is quite different from that with the forward KL constraint.

A straight forward calculation shows (Appendix B): DISPLAYFORM4 The equation FORMULA1 has an intuitive interpretation.

It increases the probability of action a if A π θ k (s, a) > λ (s) − 1 and decreases the probability of action a if A π θ k (s, a) < λ (s) − 1. (22) also tries to keep π θ close to π θ k by minimizing their KL divergence.

In this section we show how a PPO-like objective can be formulated in the context of SPU.

Recall from Section 3 that the the clipping in PPO can be seen as an attempt at keeping π θ (a i |s i ) from becoming neither much larger than (1 + )π θ k (a i |s i ) nor much smaller than (1 − )π θ k (a i |s i ) for i = 1, . . .

, m. In this subsection, we consider the constraint function DISPLAYFORM0 which leads us to the following optimization problem: DISPLAYFORM1 DISPLAYFORM2 Note that here we are using a variation of the SPU methodology described in Section 4 since here we first create estimates of the expectations in the objective and constraints and then solve the optimization problem (rather than first solve the optimization problem and then take samples as done for Theorems 1 and 2).

Note that we have also included an aggregated constraint (26) in addition to the PPO-like constraint (25), which further ensures that the updated policy is close to π θ k .

The optimal solution to the optimization problem (24-26) is given by: DISPLAYFORM0 for some λ > 0 where DISPLAYFORM1 To simplify the algorithm, we treat λ as a hyper-parameter rather than δ.

After solving for π * , we seek a parameterized policy π θ that is close to π * by minimizing their mean square error over sampled states and actions, i.e. by updating θ in the negative direction of ∇ θ i (π θ (a i |s i ) − π * (a i |s i )) 2 .

This loss is used for supervised training instead of the KL because we take estimates before forming the optimization problem.

Thus, the optimal values for the decision variables do not completely characterize a distribution.

We refer to this approach as SPU with the L ∞ constraint.

Although we consider three classes of proximity constraint, there may be yet another class that leads to even better performance.

The methodology allows researchers to explore other proximity constraints in the future.

Extensive experimental results demonstrate SPU outperforms recent state-of-the-art methods for environments with continuous or discrete action spaces.

We provide ablation studies to show the importance of the different algorithmic components, and a sensitivity analysis to show that SPU's performance is relatively insensitive to hyper-parameter choices.

There are two definitions we use to conclude A is more sample efficient than B: (i) A takes fewer environment interactions to achieve a pre-defined performance threshold BID10 ; (ii) the averaged final performance of A is higher than that of B given the same number environment interactions .

Implementation details are provided in Appendix D.

The Mujoco BID25 ) simulated robotics environments provided by OpenAI gym BID5 have become a popular benchmark for control problems with continuous action spaces.

In terms of final performance averaged over all available ten Mujoco environments and ten different seeds in each, SPU with L ∞ constraint (Section 5.3) and SPU with forward KL constraints (Section 5.1) outperform TRPO by 6% and 27% respectively.

Since the forward-KL approach is our best performing approach, we focus subsequent analysis on it and hereafter refer to it as SPU.

SPU also outperforms PPO by 17%.

FIG0 illustrates the performance of SPU versus TRPO, PPO.To ensure that SPU is not only better than TRPO in terms of performance gain early during training, we further retrain both policies for 3 million timesteps.

Again here, SPU outperforms TRPO by 28%.

FIG3 in the Appendix illustrates the performance for each environment.

Code for the Mujoco experiments is at https://github.com/quanvuong/Supervised_Policy_Update.

The indicator variable in (18) enforces the disaggregated constraint.

We refer to it as per-state acceptance.

Removing this component is equivalent to removing the indicator variable.

We refer to using i D KL (π θ π θ k )[s i ] to determine the number of training epochs as dynamic stopping.

Without this component, the number of training epochs is a hyper-parameter.

We also tried removing DISPLAYFORM0 ] from the gradient update step in (18).

TAB0 illustrates the contribution of the different components of SPU to the overall performance.

The third row shows that the term DISPLAYFORM1 ] makes a crucially important contribution to SPU.

Furthermore, per-state acceptance and dynamic stopping are both also important for obtaining high performance, with the former playing a more central role.

When a component is removed, the hyper-parameters are retuned to ensure that the best possible performance is obtained with the alternative (simpler) algorithm.

To demonstrate the practicality of SPU, we show that its high performance is insensitive to hyperparameter choice.

One way to show this is as follows: for each SPU hyper-parameter, select a reasonably large interval, randomly sample the value of the hyper parameter from this interval, and then compare SPU (using the randomly chosen hyper-parameter values) with TRPO.

We sampled 100 SPU hyper-parameter vectors (each vector including δ, , λ), and for each one determined the relative performance with respect to TRPO.

First, we found that for all 100 random hyper-parameter value samples, SPU performed better than TRPO.

75% and 50% of the samples outperformed TRPO by at least 18% and 21% respectively.

The full CDF is given in Figure 4 in the Appendix.

We can conclude that SPU's superior performance is largely insensitive to hyper-parameter values.6.4 RESULTS ON ATARI BID20 BID15 demonstrates that neural networks are not needed to obtain high performance in many Mujoco environments.

To conclusively evaluate SPU, we compare it against PPO on the Arcade Learning Environments (Bellemare et al., 2012) exposed through OpenAI gym BID5 .

Using the same network architecture and hyper-parameters, we learn to play 60 Atari games from raw pixels and rewards.

This is highly challenging because of the diversity in the games and the high dimensionality of the observations.

Here, we compare SPU against PPO because PPO outperforms TRPO by 9% in Mujoco.

Averaged over 60 Atari environments and 20 seeds, SPU is 55% better than PPO in terms of averaged final performance.

FIG1 provides a high-level overview of the result.

The dots in the shaded area represent environments where their performances are roughly similar.

The dots to the right of the shaded area represent environment where SPU is more sample efficient than PPO.

We can draw two conclusions: (i) In 36 environments, SPU and PPO perform roughly the same ; SPU clearly outperforms PPO in 15 environments while PPO clearly outperforms SPU in 9; (ii) In those 15+9 environments, the extent to which SPU outperforms PPO is much larger than the extent to which PPO outperforms SPU.

FIG5 , Figure 6 and FIG6 in the Appendix illustrate the performance of SPU vs PPO throughout training.

SPU's high performance in both the Mujoco and Atari domains demonstrates its high performance and generality.

We first show that FORMULA0 - FORMULA0 is a convex optimization.

To this end, first note that the objective FORMULA0 is a linear function of the decision variables π = {π(a|s): s ∈ S, a ∈ A}. The LHS of FORMULA0 can be rewritten as: a∈A π(a|s) log π(a|s) − a∈A π(a|s) log π θ k (a|s).

The second term is a linear function of π.

The first term is a convex function since the second derivative of each summand is always positive.

The LHS of FORMULA0 is thus a convex function.

By extension, the LHS of FORMULA0 is also a convex function since it is a nonnegative weighted sum of convex functions.

The problem FORMULA0 - FORMULA0 is thus a convex optimization problem.

According to Slater's constraint qualification, strong duality holds since π θ k is a feasible solution to FORMULA0 - FORMULA0 where the inequality holds strictly.

We can therefore solve FORMULA0 - FORMULA0 by solving the related Lagrangian problem.

For a fixed λ consider: DISPLAYFORM0 The above problem decomposes into separate problems, one for each state s: DISPLAYFORM1 DISPLAYFORM2 Further consider the unconstrained problem (30) without the constraint (31): DISPLAYFORM3 subject to DISPLAYFORM4 A simple Lagrange-multiplier argument shows that the opimal solution to FORMULA1 - FORMULA2 is given by: DISPLAYFORM5 where Z λ (s) is defined so that π λ (·|s) is a valid distribution.

Now returning to the decomposed constrained problem (30)-(31), there are two cases to consider.

The first case is when DISPLAYFORM6 In this case, the optimal solution to (30)- FORMULA0 is π λ (a|s).

The second case is when DISPLAYFORM7 In this case the optimal is π λ (a|s) with λ replaced with λ s , where λ s is the solution to DISPLAYFORM8 Thus, an optimal solution to (30)-(31) is given by: DISPLAYFORM9 where DISPLAYFORM10 To find the Lagrange multiplier λ, we can then do a line search to find the λ that satisfies: DISPLAYFORM11 A.2 BACKWARD KL CONSTRAINTThe problem FORMULA0 - FORMULA1 decomposes into separate problems, one for each state s ∈ S: DISPLAYFORM12 subject to E DISPLAYFORM13 After some algebra, we see that above optimization problem is equivalent to: DISPLAYFORM14 DISPLAYFORM15 where = + entropy(π θ k ).

FORMULA2 - FORMULA1 is a convex optimization problem with Slater's condition holding.

Strong duality thus holds for the problem FORMULA2 - FORMULA1 .

Applying standard Lagrange multiplier arguments, it is easily seen that the solution to FORMULA2 - FORMULA1 is DISPLAYFORM16 where λ(s) and λ (s) are constants chosen such that the disaggregegated KL constraint is binding and the sum of the probabilities equals 1.

It is easily seen λ(s) > 0 and DISPLAYFORM17 The problem (24-26) is equivalent to: DISPLAYFORM18 DISPLAYFORM19 This problem is clearly convex.

π θ k (a i |s i ), i = 1, . . .

, m is a feasible solution where the inequality constraint holds strictly.

Strong duality thus holds according to Slater's constraint qualification.

To solve FORMULA2 - FORMULA3 , we can therefore solve the related Lagrangian problem for fixed λ: DISPLAYFORM20 DISPLAYFORM21 which is separable and decomposes into m separate problems, one for each s i : DISPLAYFORM22 DISPLAYFORM23 DISPLAYFORM24 λ s (Log of product is sum of log) DISPLAYFORM25 (Adding the gradient of the entropy on both sides and collapse the sum of gradients of cross entropy and entropy into the gradient of the KL) DISPLAYFORM26 (Taking gradient on both sides) DISPLAYFORM27 (Adding the gradient of the entropy on both sides and collapse the sum of gradients of cross entropy and entropy into the gradient of the KL)

The methodology developed in the body of this paper also applies to continuous state and action spaces.

In this section, we outline the modifications that are necessary for the continuous case.

We first modify the definition of d π (s) by replacing P π (s t = s) with d ds P π (s t ≤ s) so that d π (s) becomes a density function over the state space.

With this modification, the definition ofD KL (π π k ) and the approximation (8) are unchanged.

The SPU framework described in Section 4 is also unchanged.

Consider now the non-parameterized optimization problem with aggregate and disaggregate constraints (12-14), but with continuous state and action space: DISPLAYFORM0 Theorem 1 holds although its proof needs to be slightly modified as follows.

It is straightforward to show that (50-52) remains a convex optimization problem.

We can therefore solve (50-52) by solving the Lagrangian (28-29) with the sum replaced with an integral.

This problem again decomposes with separate problems for each s ∈ S giving exactly the same equations (30-31).

The proof then proceeds as in the remainder of the proof of Theorem 1.Theorem 2 and 3 are also unchanged for continuous action spaces.

Their proofs require slight modifications, as in the proof of Theorem 1.

As in , for Mujoco environments, the policy is parameterized by a fullyconnected feed-forward neural network with two hidden layers, each with 64 units and tanh nonlinearities.

The policy outputs the mean of a Gaussian distribution with state-independent variable standard deviations, following BID21 BID7 .

The action dimensions are assumed to be independent.

The probability of an action is given by the multivariate Gaussian probability distribution function.

The baseline used in the advantage value calculation is parameterized by a similarly sized neural network, trained to minimize the MSE between the sampled states TD−λ returns and the their predicted values.

For both the policy and baseline network, SPU and TRPO use the same architecture.

To calculate the advantage values, we use Generalized Advantage Estimation BID22 .

States are normalized by dividing the running mean and dividing by the running standard deviation before being fed to any neural networks.

The advantage values are normalized by dividing the batch mean and dividing by the batch standard deviation before being used for policy update.

The TRPO result is obtained by running the TRPO implementation provided by OpenAI , commit 3cc7df060800a45890908045b79821a13c4babdb.

At every iteration, SPU collects 2048 samples before updating the policy and the baseline network.

For both networks, gradient descent is performed using Adam (Kingma & Ba, 2014) with step size 0.0003, minibatch size of 64.

The step size is linearly annealed to 0 over the course of training.

γ and λ for GAE BID22 BID17 .

The output of the network is passed through a relu, linear and softmax layer in that order to give the action distribution.

The output of the network is also passed through a different linear layer to give the baseline value.

States are normalized by dividing by 255 before being fed into any network.

The TRPO result is obtained by running the PPO implementation provided by OpenAI , commit 3cc7df060800a45890908045b79821a13c4babdb.

8 different processes run in parallel to collect timesteps.

At every iteration, each process collects 256 samples before updating the policy and the baseline network.

Each process calculates its own update to the network's parameters and the updates are averaged over all processes before being used to update the network's parameters.

Gradient descent is performed using Adam (Kingma & Ba, 2014) with step size 0.0001.

In each process, random number generators are initialized with a different seed according to the formula process_seed = experiment_seed + 10000 * process_rank.

Training is performed for 10 million timesteps for both SPU and PPO.

For SPU, δ, , λ and the maximum number of epochs per iteration are set to 0.02, δ/1.3, 1.1 and 9 respectively.

Algorithm 1 Algorithmic description of forward-KL non-parameterized SPU Require: A neural net π θ that parameterizes the policy.

Require: A neural net V φ that approximates V π θ .

Require: General hyperparameters: γ, β (advantage estimation using GAE), α (learning rate), N (number of trajectory per iteration), T (size of each trajectory), M (size of training minibatch).

Require: Algorithm-specific hyperparameters: δ (aggregated KL constraint), (disaggregated constraint), λ, ζ (max number of epoch).

1: for k = 1, 2, . . .

do

under policy π θ k , sample N trajectories, each of size T (s it , a it , r it , s i(t+1) ), i = 1, . . .

, N, t = 1, . . .

, T

Using any advantage value estimation scheme, estimate A it , i = 1, . . .

, N, t = 1, . . . , T DISPLAYFORM0 10: DISPLAYFORM1 θ ← θ − αL(θ) TRPO and SPU were trained for 1 million timesteps to obtain the results in section 6.

To ensure that SPU is not only better than TRPO in terms of performance gain early during training, we further retrain both policies for 3 million timesteps.

Again here, SPU outperforms TRPO by 28%.

FIG3 illustrates the performance on each environment.

When values for SPU hyper-parameter are randomly sampled as is explained in subsection 6.3, the percentage improvement of SPU over TRPO becomes a random variable.

Figure 4 illustrates the CDF of this random variable.

<|TLDR|>

@highlight

first posing and solving the sample efficiency optimization problem in the non-parameterized policy space, and then solving a supervised regression problem to find a parameterized policy that is near the optimal non-parameterized policy.
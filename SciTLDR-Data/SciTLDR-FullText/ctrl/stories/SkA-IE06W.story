We analyze the convergence of (stochastic) gradient descent algorithm for learning a convolutional filter with Rectified Linear Unit (ReLU) activation function.

Our analysis does not rely on any specific form of the input distribution and our proofs only use the definition of ReLU, in contrast with previous works that are restricted to standard Gaussian input.

We show that (stochastic) gradient descent with random initialization can learn the convolutional filter in polynomial time and the convergence rate depends on the smoothness of the input distribution and the closeness of patches.

To the best of our knowledge, this is the first recovery guarantee of gradient-based algorithms for convolutional filter on non-Gaussian input distributions.

Our theory also justifies the two-stage learning rate strategy in deep neural networks.

While our focus is theoretical, we also present experiments that justify our theoretical findings.

Deep convolutional neural networks (CNN) have achieved the state-of-the-art performance in many applications such as computer vision BID16 , natural language processing BID3 and reinforcement learning applied in classic games like Go BID31 .

Despite the highly non-convex nature of the objective function, simple first-order algorithms like stochastic gradient descent and its variants often train such networks successfully.

On the other hand, the success of convolutional neural network remains elusive from an optimization perspective.

When the input distribution is not constrained, existing results are mostly negative, such as hardness of learning a 3-node neural network BID0 or a non-overlap convolutional filter BID1 .

Recently, BID30 showed learning a simple one-layer fully connected neural network is hard for some specific input distributions.

These negative results suggest that, in order to explain the empirical success of SGD for learning neural networks, stronger assumptions on the input distribution are needed.

Recently, a line of research BID36 BID1 BID18 BID33 BID39 assumed the input distribution be standard Gaussian N (0, I) and showed (stochastic) gradient descent is able to recover neural networks with ReLU activation in polynomial time.

One major issue of these analysis is that they rely on specialized analytic properties of the Gaussian distribution (c.f.

Section 1.1) and thus cannot be generalized to the non-Gaussian case, in which real-world distributions fall into.

For general input distributions, new techniques are needed.

In this paper we consider a simple architecture: a convolution layer, followed by a ReLU activation function, and then average pooling.

Formally, we let x ∈ R d be an input sample, e.g., an image, we generate k patches from x, each with size p: Z ∈ R p×k where the i-th column is the i-th patch generated by some known function Z i = Z i (x).

For a filter with size 2 and stride 1, Z i (x) is the i-th and (i + 1)-th pixels.

Since for convolutional filters, we only need to focus on the patches instead of the input, in the following definitions and theorems, we will refer Z as input and let Z as the distribution of Z: (σ(x) = max(x, 0) is the ReLU activation function) (a) (b) (c) Figure 1 :

(a) Architecture of the network we are considering.

Given input X, we extract its patches {Z i } and send them to a shared weight vector w.

The outputs are then sent to ReLU and then summed to yield the final label (and its estimation).

(b)-(c) Two conditions we proposed for convergence.

We want the data to be (b) highly correlated and (c) concentrated more on the direction aligned with the ground truth vector w * .

DISPLAYFORM0 See Figure 1 (a) for a graphical illustration.

Such architectures have been used as the first layer of many works in computer vision BID19 BID23 .

We address the realizable case, where training data are generated from (1) with some unknown teacher parameter w * under input distribution Z. Consider the 2 loss (w, Z) = 1 2 (f (w, Z) − f (w * , Z)) 2 .

We learn by (stochastic) gradient descent, i.e., DISPLAYFORM1 where η t is the step size which may change over time and g(w t ) is a random function where its expectation equals to the population gradient E [g(w)] = E Z∼Z [∇ (w, Z)] .

The goal of our analysis is to understand the conditions where w → w * , if w is optimized under (stochastic) gradient descent.

In this setup, our main contributions are as follows:• Learnability of Filters: We show if the input patches are highly correlated (Section 3), i.e., θ (Z i , Z j ) ≤ ρ for some small ρ > 0, then gradient descent and stochastic gradient descent with random initialization recovers the filter in polynomial time.

1 Furthermore, strong correlations imply faster convergence.

To the best of our knowledge, this is the first recovery guarantee of randomly initialized gradient-based algorithms for learning filters (even for the simplest one-layer one-neuron network) on non-Gaussian input distribution, answering an open problem in BID36 .•

Distribution-Aware Convergence Rate.

We formally establish the connection between the smoothness of the input distribution and the convergence rate for filter weights recovery where the smoothness in our paper is defined as the ratio between the largest and the least eigenvalues of the second moment of the activation region (Section 2).

We show that a smoother input distribution leads to faster convergence, and Gaussian distribution is a special case that leads to the tightest bound.

This theoretical finding also justifies the twostage learning rate strategy proposed by BID12 BID35 if the step size is allowed to change over time.

In recent years, theorists have tried to explain the success of deep learning from different perspectives.

From optimization point of view, optimizing neural network is a non-convex optimization problem.

Pioneered by BID7 , a class of non-convex optimization problems that satisfy strict saddle property can be optimized by perturbed (stochastic) gradient descent in polynomial time .

2 This motivates the research of studying the landscape of neural networks BID15 BID2 BID11 BID10 BID22 BID5 BID26 BID40 BID24 However, these results cannot be directly applied to analyzing the convergence of gradient-based methods for ReLU activated neural networks.

From learning theory point of view, it is well known that training a neural network is hard in the worst cases BID0 BID21 Šíma, 2002; BID28 b) and recently, BID30 showed either "niceness" of the target function or of the input distribution alone is sufficient for optimization algorithms used in practice to succeed.

With some additional assumptions, many works tried to design algorithms that provably learn a neural network with polynomial time and sample complexity BID9 BID27 BID13 BID6 BID8 .

However, these algorithms are tailored for certain architecture and cannot explain why (stochastic) gradient based optimization algorithms work well in practice.

Focusing on gradient-based algorithms, a line of research analyzed the behavior of (stochastic) gradient descent for Gaussian input distribution.

BID36 showed population gradient descent is able to find the true weight vector with random initialization for one-layer one-neuron model.

BID1 showed population gradient descent recovers the true weights of a convolution filter with non-overlapping input in polynomial time.

BID18 showed SGD can recover the true weights of a one-layer ResNet model with ReLU activation under the assumption that the spectral norm of the true weights is bounded by a small constant.

All the methods use explicit formulas for Gaussian input, which enable them to apply trigonometric inequalities to derive the convergence.

With the same Gaussian assumption, BID33 shows that the true weights can be exactly recovered by projected gradient descent with enough samples in linear time, if the number of inputs is less than the dimension of the weights.

Other approaches combine tensor approaches with assumptions of input distribution.

BID39 proved that with sufficiently good initialization, which can be implemented by tensor method, gradient descent can find the true weights of a 3-layer fully connected neural network.

However, their approach works with known input distributions.

BID33 used Gaussian width (c.f.

Definition 2.2 of BID33 ) for concentrations and his approach cannot be directly extended to learning a convolutional filter.

In this paper, we adopt a different approach that only relies on the definition of ReLU.

We show as long as the input distribution satisfies weak smoothness assumptions, we are able to find the true weights by SGD in polynomial time.

Using our conclusions, we can justify the effectiveness of large amounts of data (which may eliminate saddle points), two-stage and adaptive learning rates used by BID12 ; BID35 , etc.

This paper is organized as follows.

In Section 2, we analyze the simplest one-layer one-neuron model where we state our key observation and establish the connection between smoothness and convergence rate.

In Section 3, we discuss the performance of (stochastic) gradient descent for learning a convolutional filter.

We provide empirical illustrations in Section 4 and conclude in Section 5.

We place most of our detailed proofs in the Appendix.

Let · 2 denote the Euclidean norm of a finite-dimensional vector.

For a matrix A, we use λ max (A) to denote its largest singular value and λ min (A) its smallest singular value.

Note if A is a positive semidefinite matrix, λ max (A) and λ min (A) represent the largest and smallest eigenvalues of A, respectively.

Let O(·) and Θ(·) denote the standard Big-O and Big-Theta notations that hide absolute constants.

We assume the gradient function is uniformly bounded, i.e., There exists B > 0 such that g(w) 2 ≤ B. This condition is satisfied as long as patches, w and noise are all bounded.

DISPLAYFORM0

Before diving into the convolutional filter, we first analyze the special case for k = 1, which is equivalent to the one-layer one-neuron architecture.

The analysis in this simple case will give us insights for the fully general case.

For the ease of presentation, we define following two events and corresponding second moments DISPLAYFORM0 where I {·} is the indicator function.

Intuitively, S(w, w * ) is the joint activation region of w and w * and S(w, −w * ) is the joint activation region of w and −w * .

See FIG1 (a) for the graphical illustration.

With some simple algebra we can derive the population gradient.

DISPLAYFORM1 One key observation is we can write the inner product ∇ w (w) , w − w * as the sum of two non-negative terms (c.f.

Lemma A.1).

This observation directly leads to the following Theorem 2.1.Theorem 2.1.

Suppose for any w 1 , w 2 with θ (w 1 , w 2 ) < π, E ZZ I {S(w, w * )} 0 and the initialization w 0 satisfies (w 0 ) < (0) then gradient descent algorithm recovers w * .The first assumption is about the non-degeneracy of input distribution.

For θ (w 1 , w 2 ) < π, one case that the assumption fails is that the input distribution is supported on a low-dimensional space, or degenerated.

The second assumption on the initialization is to ensure that gradient descent does not converge to w = 0, at which the gradient is undefined.

This is a general convergence theorem that holds for a wide class of input distribution and initialization points.

In particular, it includes Theorem 6 of BID36 as a special case.

If the input distribution is degenerate, i.e., there are holes in the input space, the gradient descent may stuck around saddle points and we believe more data are needed to facilitate the optimization procedure This is also consistent with empirical evidence in which more data are helpful for optimization.

In the previous section we showed if the distribution is regular and the weights are initialized appropriately, gradient descent recovers the true weights when it converges.

In practice we also want to know how many iterations are needed.

To characterize the convergence rate, we need some quantitative assumptions.

We note that different set of assumptions will lead to a different rate and ours is only one possible choice.

In this paper we use the following quantities.

DISPLAYFORM0 These two conditions quantitatively characterize the angular smoothness of the input distribution.

For a given angle φ, if the difference between γ(φ) and L(φ) is large then there is one direction has large probability mass and one direction has small probability mass, meaning the input distribution is not smooth.

On the other hand, if γ(φ) and L(φ) are close, then all directions have similar probability mass, which means the input distribution is smooth.

The smoothest input distributions are rotationally invariant distributions (e.g. standard Gaussian) which have γ(φ) = L(φ).

For analogy, we can think of L(φ) as Lipschitz constant of the gradient and γ(φ) as the strong convexity parameter in the optimization literature but here we also allow they change with the angle.

Also observe that when φ = π, γ(φ) = L(φ) = 0 because the intersection has measure 0 and both γ(φ) and L(φ) are monotonically decreasing.

Our next assumption is on the growth of A w,−w * .

Note that when θ (w, w * ) = 0, then A w,−w * = 0 because the intersection between w and −w * has 0 measure.

Also, A w,−w * grows as the angle between w and w * becomes larger.

In the following, we assume the operator norm of A w,−w * increases smoothly with respect to the angle.

The intuition is that as long as input distribution bounded probability density with respect to the angle, the operator norm of A w,−w * is bounded.

We show in Theorem A.1 that β = 1 for rotational invariant distribution and in Theorem A.2 that β = p for standard Gaussian distribution.

Assumption 2.1.

We assume there exists β > 0 that for DISPLAYFORM1 Now we are ready to state the convergence rate.

Theorem 2.2.

Suppose the initialization w 0 satisfies w 0 − w * 2 < w * 2 .

Denote φ t = arcsin wt−w * 2 w * 2 then if step size is set as 0 ≤ η t ≤ min 0≤φ≤φt DISPLAYFORM2 2 , we have for t = 1, 2, . . .

DISPLAYFORM3 Note both γ(φ) and L(φ) increases as φ decreases so we can choose a constant step size DISPLAYFORM4 .

This theorem implies that we can find the -close solution of w * in DISPLAYFORM5 iterations.

It also suggests a direct relation between the smoothness of the distribution and the convergence rate.

For smooth distribution where γ(φ) and L(φ) are close and β is small then DISPLAYFORM6 is relatively small and we need fewer iterations.

On the other hand, if L(φ) or β is much larger than γ(φ), we will need more iterations.

We verify this intuition in Section 4.If we are able to choose the step sizes adaptively DISPLAYFORM7 , like using methods proposed by BID20 , we may improve the computational complexity to O max φ≤φ0 DISPLAYFORM8 .

This justifies the use of two-stage learning rate strategy proposed by BID12 ; BID35 where at the beginning we need to choose learning to be small because DISPLAYFORM9 2 is small and later we can choose a large learning rate because as the angle between w t and w * becomes smaller, DISPLAYFORM10 The theorem requires the initialization satisfying w 0 − w * 2 < w * 2 , which can be achieved by random initialization with constant success probability.

See Section 3.2 for a detailed discussion.

In this section we generalize ideas from the previous section to analyze the convolutional filter.

First, for given w and w * we define four events that divide the input space of each patch Z i .

Each event corresponds to a different activation region induced by w and w * , similar to (3).

DISPLAYFORM0 Please check FIG1 (a) again for illustration.

For the ease of presentation we also define the average over all patches in each region DISPLAYFORM1 Next, we generalize the smoothness conditions analogue to Definition 2.1 and Assumption 2.1.

Here the smoothness is defined over the average of patches.

DISPLAYFORM2 We assume for all 0 ≤ φ ≤ π/2, max w:θ(w,w * )=φ λ max E Z S(w,−w * ) Z S(w,−w * ) ≤ βφ for some β > 0.The main difference between the simple one-layer one-neuron network and the convolution filter is two patches may appear in different regions.

For a given sample, there may exists patch Z i and Z j such that Z i ∈ S(w, w * ) i and Z j ∈ S(w, −w * ) j and their interaction plays an important role in the convergence of (stochastic) gradient descent.

Here we assume the second moment of this interaction, i.e., cross-covariance, also grows smoothly with respect to the angle.

Assumption 3.2.

We assume there exists L cross > 0 such that max w:θ(w,w * )≤φ λ max E Z S(w,w * ) Z S(w,−w * ) +λ max E Z S(w,w * ) Z S(−w,w * ) DISPLAYFORM3 First note if φ = 0, then Z S(w,−w * ) and Z S(−w,w * ) has measure 0 and this assumption models the growth of cross-covariance.

Next note this L cross represents the closeness of patches.

If Z i and Z j are very similar, then the joint probability density of Z i ∈ S(w, w * ) i and Z j ∈ S(w, −w * ) j is small which implies L cross is small.

In the extreme setting, DISPLAYFORM4 Now we are ready to present our result on learning a convolutional filter by gradient descent.

Theorem 3.1.

If the initialization satisfies w 0 − w * 2 < w * 2 and denote φ t = arcsin wt−w * 2 w * 2 which satisfies γ(φ 0 ) > 6L cross .

Then if we choose η t ≤ min 0≤φ≤φt DISPLAYFORM5 2(L(φ)+10Lcross+4β) 2 , we have for t = 1, 2, . . .

and φ t arcsin wt−w * 2 w * 2 DISPLAYFORM6 Our theorem suggests if the initialization satisfies γ(φ 0 ) > 6L cross , we obtain linear convergence rate.

In Section 3.1, we give a concrete example showing closeness of patches implies large γ(φ) and small L cross .

Similar to Theorem 2.2, if the step size is chosen so that DISPLAYFORM7 iterations, we can find the -close solution of w * and the proof is also similar to that of Theorem 3.1.In practice,we never get a true population gradient but only stochastic gradient g(w) (c.f.

Equation FORMULA1 ).

The following theorem shows SGD also recovers the underlying filter.

iterations, with probability at least 1 − δ we have w T − w * ≤ w * 2 .Unlike the vanilla gradient descent case, here the convergence rate depends on φ 1 instead of φ 0 .

This is because of the randomness in SGD and we need a more robust initialization.

We choose φ 1 to be the average of φ 0 and φ * for the ease of presentation.

As will be apparent in the proof we only require φ 0 not very close to φ * .

The proof relies on constructing a martingale and use Azuma-Hoeffding inequality and this idea has been previously used by BID7 .

Different from One-Layer One-Neuron model, here we also requires the Lipschitz constant for closeness L cross to be relatively small and γ(φ 0 ) to be relatively large.

A natural question is: What input distributions satisfy this condition?Here we give an example.

We show if (1) patches are close to each other (2) the input distribution has small probability mass around the decision boundary then the assumption in Theorem 3.1 is satisfied.

See Figure 1 DISPLAYFORM0 then we have DISPLAYFORM1 where γ avg (φ 0 ) = σ min E ZZ I w 0 Z ≥ 0, w * Z ≥ 0 , analogue to Definition 2.1.Several comments are in sequel.

We view ρ as a quantitative measure of the closeness between different patches, i.e., ρ small means they are similar.

This lower bound is monotonically decreasing as a function of ρ and note when ρ = 0, σ min E Z S(w,w * ) Z S(w,w * ) = γ avg (φ 0 ) which recovers Definition 2.1.For the upper bond on L cross , µ represents the upper bound of the probability density around the decision boundary.

DISPLAYFORM2 φ.

This assumption is usually satisfied in real world examples like images because the image patches are not usually close to the decision boundary.

For example, in computer vision, the local image patches often form clusters and is not evenly distributed over the appearance space.

Therefore, if we use linear classifier to separate their cluster centers from the rest of the clusters, near the decision boundary the probability mass should be very low.

For one-layer one-neuron model, we need initialization w 0 − w * 2 < w * 2 and for the convolution filter, we need a stronger initialization w 0 − w * 2 < w * 2 cos (φ * ).

The following theorem shows with uniformly random initialization we have constant probability to obtain a good initialization.

Note with this theorem at hand, we can boost the success probability to arbitrary close to 1 by random restarts.

The proof is similar to BID36 .Theorem 3.4.

If we uniformly sample w 0 from a p-dimensional ball with radius α w * so that α ≤ 1 2πp , then with probability at least DISPLAYFORM0 To apply this general initialization theorem to our convolution filter case, we can choose α = cos φ * .

Therefore, with some simple algebra we have the following corollary.

DISPLAYFORM1 , then if w 0 is uniformly sampled from a ball with center 0 and radius w * cos (φ * ), we have with probability at least The assumption of this corollary is satisfied if the patches are close to each other as discussed in the previous section.

In this section we use simulations to verify our theoretical findings.

We first test how the smoothness affect the convergence rate in one-layer one-neuron model described in Section 2 To construct input distribution with different L(φ), γ(φ) and β (c.f.

Definition 2.1 and Assumption 2.1), we fix the patch to have unit norm and use a mixture of truncated Gaussian distribution to model on the angle around w * and around the −w * Specifically, the probability density of ∠Z, w * is sampled from DISPLAYFORM0 Note by definitions of L(φ) and γ(φ) if σ → 0 the probability mass is centered around w * , so the distribution is very spiky and L(φ)/γ(φ) and β will be large.

On the other hand, if σ → ∞, then input distribution is close to the rotation invariant distribution and L(φ)/γ(φ) and β will be small.

FIG7 verifies our prediction where we fix the initialization and step size.

Next we test how the closeness of patches affect the convergence rate in the convolution setting.

We first generate a single patch Z using the above model with σ = 1, then generate each unit norm Z i whose angle with Z, ∠Z i , Z is sampled from ∠Z i , Z ∼ N (0, σ 2 )I [−π,π) .

FIG7 shows as variance between patches becomes smaller, we obtain faster convergence rate, which coincides with Theorem 3.1.We also test whether SGD can learn a filter on real world data.

Here we choose MNIST data and generate labels using two filters.

One is random filter where each entry is sampled from a standard Gaussian distribution FIG8 ) and the other is a Gabor filter FIG8 ).

FIG7 and FIG7 show convergence rates of SGD with different initializations.

Here, better initializations give faster rates, which coincides our theory.

Note that here we report the relative loss, logarithm of squared error divided by the square of mean of data points instead of the difference between learned filter and true filter because we found SGD often cannot converge to the exact filter but rather a filter with near zero loss.

We believe this is because the data are approximately lying in a low dimensional manifold in which the learned filter and the true filter are equivalent.

To justify this conjecture, we try to interpolate the learned filter and the true filter linearly and the result filter has similar low loss (c.f.

FIG12 .

Lastly, we visualize the true filters and the learned filters in FIG8 and we can see that the they have similar patterns.

In this paper we provide the first recovery guarantee of (stochastic) gradient descent algorithm with random initialization for learning a convolution filter when the input distribution is not Gaussian.

Our analyses only used the definition of ReLU and some mild structural assumptions on the input distribution.

Here we list some future directions.

One possibility is to extend our result to deeper and wider architectures.

Even for two-layer fullyconnected network, the convergence of (stochastic) gradient descent with random initialization is not known.

Existing results either requires sufficiently good initialization BID39 relies on special architecture BID18 .

However, we believe the insights from this paper is helpful to understand the behaviors of gradient-based algorithms in these settings.

Another direction is to consider the agnostic setting, where the label is not equal to the output of a neural network.

This will lead to different dynamics of (stochastic) gradient descent and we may need to analyze the robustness of the optimization procedures.

This problem is also related to the expressiveness of the neural network BID25 where if the underlying function is not equal bot is close to a neural network.

We believe our analysis can be extend to this setting.

Lemma A.1.

DISPLAYFORM0 and both terms are non-negative.

Proof.

Since A w,w * 0 and A w,−w * 0 (positive-semidefinite), both the first term and one part of the second term w A w,−w * w are non-negative.

The other part of the second term is DISPLAYFORM1 Proof of Theorem 2.1.

The assumption on the input distribution ensures when θ (w, w * ) = π, A w,w * 0 and when θ (w, w * ) = 0, A w,−w * 0.

Now when gradient descent converges we have ∇ w (w) = 0.

We have the following theorem.

By assumption, since (w) < (0) and gradient descent only decreases function value, we will not converge to w = 0.

Note that at any critical points, ∇ w (w) , w − w * = 0, from Lemma A.1, we have: DISPLAYFORM2 Suppose we are converging to a critical point w = w * .

There are two cases:• If θ (w, w * ) = π, then we have(w − w * ) A w,w * (w − w * ) > 0, which contradicts with Eqn.

6.• If θ (w, w * ) = π, without loss of generality, let w = −αw * for some α > 0.

By the assumption we know A w,−w * 0.

Now the second equation becomes (w − w * ) A w,−w * w = (1 + γ)w * A w,−w * w * > 0, which contradicts with Eqn.

7.Therefore we have w = w * .Proof of Theorem 2.2.

Our proof relies on the following simple but crucial observation: if w − w * 2 < w * 2 , then DISPLAYFORM3 We denote θ (w t , w * ) = θ t and by the observation we have θ t ≤ φ t .

Recall the gradient descent dynamics, DISPLAYFORM4 Consider the squared distance to the optimal weight DISPLAYFORM5 By our analysis in the previous section, the second term is smaller than DISPLAYFORM6 where we have used our assumption on the angle.

For the third term, we expand it as DISPLAYFORM7 .

Therefore, in summary, DISPLAYFORM8 where the first inequality is by our assumption of the step size and second is because θ t ≤ φ t and γ(·) is monotonically decreasing.

Theorem A.1 (Rotational Invariant Distribution).

For any unit norm rotational invariant input distribution, we have β = 1.Proof of Theorem A.1.

Without loss of generality, we only need to focus on the plane spanned by w and w * and suppose w * = (1, 0) .

Then DISPLAYFORM9 It has two eigenvalues DISPLAYFORM10 Therefore, max w,θ(w,w * )≤φ λ max (A w,−w * ) = φ+sin φ 2 DISPLAYFORM11 Proof.

Note in previous theorem we can integrate angle and radius separately then multiply them together.

For Gaussian distribution, we have E Z 2 2 ≤ p.

The result follows.

Proof of Theorem 3.1.

The proof is very similar to Theorem 2.2.

Notation-wise, for two events S 1 , S 2 we use S 1 S 2 as a shorthand for S 1 ∩ S 2 and S 1 + S 2 as a shorthand for S 1 ∪ S 2 .

Denote θ t = θ (w t , w * ) .

First note with some routine algebra, we can write the gradient as DISPLAYFORM0 We first examine the inner product between the gradient and w − w * .

DISPLAYFORM1 where the first inequality we used the definitions of the regions; the second inequality we used the definition of operator norm; the third inequality we used the fact w t − w * 2 ≤ w * 2 ; the fourth inequality we used the definition of L cross and the fifth inequality we used φ ≤ 2 sin φ for any 0 ≤ φ ≤ π/2.

Next we can upper bound the norm of the gradient using similar argument DISPLAYFORM2 Therefore, using the dynamics of gradient descent, putting the above two bounds together, we have DISPLAYFORM3 where the last step we have used our choice of η t and θ t ≤ φ t .The proof of Theorem 3.2 consists of two parts.

First we show if η is chosen properly and T is not to big, then for all 1 ≤ t ≤ T , with high probability the iterates stat in a neighborhood of w * .

Next, conditioning on this, we derive the rate.

Lemma A.2.

Denote r 0 = w 0 − w * 2 < w * 2 sin φ * .

Given 0 < r 1 < w * 2 sin φ * , number of iterations T ∈ Z ++ and failure probability δ, denote φ 1 = arcsin T (1 + 2ηαT ) (2ηB (L(0) + 10L cross + 4β) r 1 + η 2 B 2 ) 2 ≥ log T δ with α = γ (φ 1 ) − η (L(0) + 10L cross + 4β).

Then with probability at least 1 − δ, for all t = 1, . . .

, T , we have w t − w * ≤ r 1 .Proof of Lemma A.2.

Let g(w t ) = E [∇ wt (w t )] + ξ t .

We denote F t = σ {ξ 1 , . . .

, ξ t }, the sigmaalgebra generated by ξ 1 , . . . , ξ t and define the event C t = {∀τ ≤ t, w τ − w * ≤ r 1 } .

E w t+1 − w * 2 2 I Ct |F t =E w t − η∇ wt (w t ) − w * − ηξ t On the other hand, if φ ≥ γ, let θ j be the angle between w * and Z j , we have S(w,−w * ) j S(w,w * ) i dP (Z i |Z j ) dP (θ j ) ≤ π 2 +γ π 2 S(w,w * ) i dP (Z i |Z j ) dP (θ j ) ≤Lγ ≤Lφ.

Therefore, σ max E Z S(w,w * ) Z S(w,−w * ) ≤ Lφ.

Using similar arguments we can show σ max E Z S(w,w * ) Z S(−w,w * ) ≤ Lφ and σ max E Z S(w,−w * ) Z S(−w,w * ) ≤ Lφ.

Proof of Theorem 3.4.

We use the same argument by BID36 .

Let r init be the initialization radius.

The failure probability is lower bounded 1 2 (r init ) − V k (r init ) .Therefore, r init = cos (φ * ) w * 2 maximizes this lower bound.

Plugging this optimizer in and using formula for the volume of the Euclidean ball, the failure probability is lower bounded by 1 2 − cos (φ * ) πΓ (p/2 + 1) Γ (p/2 + 1/2) ≥ 1 2 − cos (φ * ) πp 2where we used Gautschi's inequality for the last step.

B ADDITIONAL EXPERIMENTAL RESULTS FIG12 show the loss of linear interpolation between the learned filter w and ground truth filter w * .

Our interpolation has the form w inter = αw + (1 − α)w * where α ∈ [0, 1] is the interpolation ratio.

Note that for all interpolation ratios, the loss remains very low.

<|TLDR|>

@highlight

We prove randomly initialized (stochastic) gradient descent learns a convolutional filter in polynomial time.

@highlight

Studies the problem of learning a single convolutional filter using SGD and shows that under certain conditions, SGD learns a single convolutional filter.

@highlight

This paper extends the Gaussian distribution assumption to a more general angular smoothness assumption, which covers a wider family of input distributions